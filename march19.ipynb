{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff167105-1856-4a04-b2df-47ed3366f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "#Ans 1: Min-Max scaling, also known as feature scaling or normalization, is a technique used in data \n",
    "#  preprocessing to scale and transform the numeric features of a dataset to a specific range. The goal is to bring\n",
    "#  all features to a common scale, typically between 0 and 1, by linearly transforming the values.\n",
    "#     This scaling method is particularly useful when working with algorithms that are sensitive to the scale of\n",
    "#     the input features, such as gradient-based optimization algorithms used in machine learning.\n",
    "\n",
    "# Here's an example in Python to illustrate Min-Max scaling using the popular scikit-learn library:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Instantiate the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the data and transform it\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90ac784-ab0e-458e-932b-a1b2dd75900e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "Unit Vector Scaled Data:\n",
      "[[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]\n",
      " [0.50257071 0.57436653 0.64616234]]\n"
     ]
    }
   ],
   "source": [
    "# Ans 2: The Unit Vector technique, also known as Unit Vector Scaling or Vector Normalization, involves\n",
    "# scaling each data point to have a magnitude of 1. This is achieved by dividing each data point by its\n",
    "# Euclidean norm. This technique ensures that all data points lie on the unit hypersphere, and it is\n",
    "# particularly useful when the direction of the data points is more important than their magnitude.\n",
    "# Min-Max scaling scales data to a specific range (e.g., between 0 and 1), whereas unit vector scaling\n",
    "# maintains the direction of the data points while ensuring they have a magnitude of 1.\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Apply Unit Vector Scaling\n",
    "unit_vector_scaled_data = normalize(data, axis=1, norm='l2')\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nUnit Vector Scaled Data:\")\n",
    "print(unit_vector_scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8654d331-993a-471c-87e7-31350b219bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "PCA Transformed Data:\n",
      "[[-5.19615242e+00  2.56395025e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Ans 3: PCA is a technique used for dimensionality reduction. It identifies the principal components, which are \n",
    "# linear combinations of the original features, and ranks them by their ability to capture the variance in the data.\n",
    "# By selecting a subset of these principal components, one can reduce the dimensionality of the dataset while \n",
    "# retaining most of the important information.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Instantiate PCA with the desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA and transform the data\n",
    "pca_transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nPCA Transformed Data:\")\n",
    "print(pca_transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf62dfe-cb82-4d26-9372-7bae7c65fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: PCA is a form of feature extraction. In feature extraction, the goal is to transform the \n",
    "# original features into a new set of features, often fewer in number, while retaining the essential\n",
    "# information present in the data. PCA achieves this by finding the principal components, which are\n",
    "# linear combinations of the original features.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0],\n",
    "                 [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Instantiate PCA with the desired number of components\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "# Fit PCA and transform the data\n",
    "feature_extracted_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nFeature Extracted Data:\")\n",
    "print(feature_extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f59df5-861d-420f-b83a-2c4eed20a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: In a recommendation system for a food delivery service, Min-Max scaling can be applied to features\n",
    "# like price, rating, and delivery time. This ensures that each of these features is on a consistent scale,\n",
    "# preventing one feature from dominating the others during the recommendation process. For example, you might \n",
    "# scale the price feature to a range between 0 and 1 as follows:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'price': [10, 20, 30, 15, 25],\n",
    "    'rating': [4.5, 3.8, 4.2, 4.8, 3.5],\n",
    "    'delivery_time': [25, 30, 20, 35, 28]\n",
    "})\n",
    "\n",
    "# Apply Min-Max scaling to the 'price' feature\n",
    "scaler = MinMaxScaler()\n",
    "data['price_scaled'] = scaler.fit_transform(data[['price']])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data[['price', 'rating', 'delivery_time']])\n",
    "print(\"\\nScaled Data:\")\n",
    "print(data[['price_scaled', 'rating', 'delivery_time']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be92220-e414-4215-a049-edd90981bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 6:In a stock price prediction project with a dataset containing numerous features, PCA can be employed\n",
    "to reduce the dimensionality and extract the most significant features. This helps in simplifying the model and\n",
    "reducing the risk of overfitting.\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with multiple features\n",
    "stock_data = pd.DataFrame(...)  # Assuming you have a DataFrame with financial and market trend data\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)  # Choose an appropriate number of components\n",
    "reduced_data = pca.fit_transform(stock_data)\n",
    "\n",
    "print(\"Original Data Shape:\", stock_data.shape)\n",
    "print(\"Reduced Data Shape:\", reduced_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207a7e9-6812-4ba9-a7c6-afeb63dad42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min-Max scaling to a range of -1 to 1\n",
    "scaled_data = (2 * (data - np.min(data)) / (np.max(data) - np.min(data))) - 1\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8132a8-ee76-4843-9338-7c2b9b03c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the number of principal components to retain depends on the desired level of variance retention.\n",
    "One common approach is to retain enough components to explain a high percentage of the total variance,\n",
    "e.g., 95% or 99%. You can analyze the explained variance ratio provided by PCA to make this decision.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame(...)  # Assuming you have a DataFrame with features\n",
    "\n",
    "# Instantiate PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA and obtain the explained variance ratio\n",
    "pca.fit(data)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Choose the number of components to retain (e.g., 95% of the variance)\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Fit PCA with the selected number of components\n",
    "pca = PCA(n_components=num_components)\n",
    "feature_extracted_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Number of Components Retained:\", num_components)\n",
    "print(\"Feature Extracted Data Shape:\", feature_extracted_data.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
